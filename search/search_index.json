{
    "docs": [
        {
            "location": "/",
            "text": "git-commit-ectomy\n\n\nPerform a git-commit-ectomy to forever remove problematic commits from your repo history.\n\n\nThis uses the \ngit-forget-blob.sh\n script from @nachoparker.\n\n\n\n\nrequirements\n\n\nThis guide utilizes GNU xargs. \nYou should run it on Linux, \nor use Homebrew's gxargs if \non a Mac.\n\n\ndemo surgery\n\n\nClone an example repo for performing surgery:\n\n\n$ git clone https://github.com/charlesreid1/git-commit-ectomy-example\n\n\n\n\nhow to make a fat file\n\n\nUse \ndd\n to create files by assembling \na specified number of bits.\nFor example, to create a 10 MB file:\n\n\n$ dd if=/dev/urandom of=my_big_fat_file bs=1048576 count=10\n\n\n\n\nImportant:\n You must use \n/dev/urandom\n with a non-zero block size.\nIf you use \n/dev/zeros\n then each file will be identical and git \nwill not store them separately. Then your surgery will go very badly.\n\n\nNote:\n \n1048576 = 2^20\n bytes comes from\n1 KB = \n2^10\n bytes, and 1 MB = \n2^10\n KB, \nfor a total of \n2^20\n bytes per megabyte.\n\n\ncount = 10 means we make 10 1mb blocks.\n\n\nmake several fat files\n\n\nCrank them out. bat cat dat fat rat!\n\n\ndd if=/dev/urandom of=bat bs=1048576 count=10\ndd if=/dev/urandom of=cat bs=1048576 count=10\ndd if=/dev/urandom of=dat bs=1048576 count=10\ndd if=/dev/urandom of=fat bs=1048576 count=10\ndd if=/dev/urandom of=rat bs=1048576 count=10\n\n\n\n\nMake sure they are the correct size:\n\n\n$ ls -lhg\n-rw-r--r--   1 staff    10M Apr 10 18:30 bat\n-rw-r--r--   1 staff    10M Apr 10 18:30 cat\n-rw-r--r--   1 staff    10M Apr 10 18:30 dat\n-rw-r--r--   1 staff    10M Apr 10 18:30 fat\n-rw-r--r--   1 staff    10M Apr 10 18:30 rat\n\n\n\n\nAlso make sure they are unique (hence \n/dev/random\n and not \n/dev/zero\n):\n\n\n$ for i in `/bin/ls -1 *at`; do\n    md5 ${i}\ndone\n\nMD5 (bat) = 140c7d1e5a12c0eb2fefd5529250a280\nMD5 (cat) = 9345ca4033c7e42fb009e3b8014570dc\nMD5 (dat) = fadc3114fe9a70f688eba0db4e0dc7a9\nMD5 (fat) = 39e98200043e438f9070369989b2d964\nMD5 (rat) = 77b1c3077041078fd1f371b1bb7dd6e4\n\n\n\n\ncommit files\n\n\nAdd the files to the repo in \nseparate commits\n:\n\n\nfor item in bat cat dat fat rat; do\n    git add ${item} && git commit ${item} -m \"Adding ${item}\"\ndone\n\ngit push origin master\n\n\n\n\nI also added two dummy files \nfoo.txt\n and \nbar.txt\n \nto illustrate the impact of the surgery on \ncommit history.\n\n\nNow you should see everything in the commit history:\n\n\n\n\nLocally in git log:\n\n\n$ git log --oneline\nfd76938 Add bar.txt\n8a86eaf Add foo.txt\nc50a272 Adding rat\n423ede3 Adding fat\nb56c10b Adding dat\nbeb8b4f Adding cat\n84d894e Adding bat\n\n\n\n\nprep for surgery\n\n\nUse \ngit-forget-blob.sh\n\nto forget the blob. Start by downloading it:\n\n\n$ wget https://tinyurl.com/git-forget-blob-sh -O git-forget-blob.sh\n$ chmod +x git-forget-blob.sh\n\n\n\n\nIf you are on a Mac, edit the file to change\n2 occurrences of \nxargs\n to \ngxargs\n.\n\n\nTo use it:\n\n\n        ./git-forget-blob.sh <name-of-file>\n\n\n\n\ngit rm\n\n\nStart by checking the size of the repo:\n\n\n$ du -hs .git\n 50M    .git\n\n\n\n\nNow remove \ndat\n using \ngit rm\n:\n\n\n$ git rm dat\n$ git commit dat -m 'Removing dat'\n$ git push origin master\n\n\n\n\nThis, of course, does not change the size of the repo.\nIf we clone a fresh copy from Github, the size of the \nrepo is still the same:\n\n\n$ du -hs .git\n 50M    .git\n\n\n\n\nGit is cursed with perfect memory, and will not\nforget a large file that's been added to the repo.\n\n\ngit forget blob\n\n\nWe use \ngit-forget-blob.sh\n to permanenty remove \n\ndat\n from the repo history and rewrite all commits\nsince the file was added:\n\n\n$ ./git-forget-blob.sh dat\nCounting objects: 23, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (22/22), done.\nWriting objects: 100% (23/23), done.\nTotal 23 (delta 6), reused 0 (delta 0)\nRewrite 84d894e39d63bbea54a7b8d3d1c85c588adff7ae (1/8) (0 seconds pRewrite beb8b4f2fbb257c0d6098620de7a8df5d7dd74ed (2/8) (0 seconds pRewrite b56c10bafeb95eece9880aa1a52cfc3e4e99045e (3/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite 423ede37f4d75dbb7a1b8020c561c4d7a926a97f (4/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite c50a2724f6d722002133b04b0f63f271b861ff9c (5/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite 8a86eafbb5b669120fa1073cfa7567bbebf2fb2e (6/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite fd769386dcd32354bad57e8eb057ae8adb2b3c9b (7/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite d0587c525a5f64d02b1cb46c04261ab285c907f9 (8/8) (0 seconds passed, remaining 0 predicted)\nRef 'refs/heads/master' was rewritten\nCounting objects: 20, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (19/19), done.\nWriting objects: 100% (20/20), done.\nTotal 20 (delta 6), reused 8 (delta 0)\n\n\n\n\nNow check if it worked:\n\n\n$ du -hs .git\n 40M    .git\n ```\n\nSuccess!\n\n## how it worked\n\nIf we check the git log we can see what happened - all commits involving `dat` were rewritten. Because git uses the prior commit's hash to make the next commit's hash, _we will rewrite every commit since the first time `dat` was added to the repo_:\n\nNew log:\n\n\n\n\n\n$ git log --oneline\n3c10144 Removing dat\n6f7b8f2 Add bar.txt\n3f70da5 Add foo.txt\n5406c1a Adding rat\ndf458d0 Adding fat\n5f6e932 Adding dat\nbeb8b4f Adding cat\n84d894e Adding bat\n\n\n\nCompare to the old log:\n\n\n\n\n\n$ git log --oneline\nd0587c5 Removing dat\nfd76938 Add bar.txt\n8a86eaf Add foo.txt\nc50a272 Adding rat\n423ede3 Adding fat\nb56c10b Adding dat\nbeb8b4f Adding cat\n84d894e Adding bat\n\n\n\nNote that the \"Add cat\" commit IDs are identical, \nbut every commit after `dat` was added to the repo is changed. \n\nThis is because each commit ID is computed using the hash of the prior commit, \nso if we change one commit ID in history, we change all subsequent commit IDs.\n\n## stitch the patient back up\n\nOf course, for surgeons, as for airline pilots,\nif the last step is screwed up, nothing else counts.\n\nWe asked git to forget a file, which it did, \nbut that required modifying commits in the \ngit history, which required re-calculating\ncommit hashes.\n\nAt this point, we have two separate, parallel \n`master` branches that split when our \nfile `dat` was added to the repo.\n\nIf we simply push our branch to the Github remote,\nwe will have a huge headache: every commit will have \na duplicate, the old branch and the new branch,\nand will show up side-by-side.\n\nTo do this correctly, we need to use the force\nwhen we push:\n\n\n\n\n\n$ git push origin master --force\n\n\n\nThis will ensure that Github does not keep duplicate\ncopies of all commits.\n\nHere is a screenshot of the repo on github before \nwe ran `git-forget-blob`:\n\n![Commit history before git-forget-blob](/img/before.png)\n\nAnd a screenshot of the repo after:\n\n![Commit history after git-forget-blob](/img/after.png)\n\n# tips for surgery\n\n**Size up your patient.**\n\nThis one-liner lists the 40 largest files in the repo\n(modify the tail line to change the number):\n\n\n\n\n\n$ git rev-list --all --objects | \\\n     sed -n $(git rev-list --objects --all | \\\n     cut -f1 -d' ' | \\\n     git cat-file --batch-check | \\\n     grep blob | \\\n     sort -n -k 3 | \\\n     \\\n     tail -n40 | \\\n     \\\n     while read hash type size; do\n          echo -n \"-e s/$hash/$size/p \";\n     done) | \\\n     sort -n -k1\n```\n\n\nGet your patient some insurance.\n Back up any files \nyou want to remove but keep.",
            "title": "Index"
        },
        {
            "location": "/#git-commit-ectomy",
            "text": "Perform a git-commit-ectomy to forever remove problematic commits from your repo history.  This uses the  git-forget-blob.sh  script from @nachoparker.",
            "title": "git-commit-ectomy"
        },
        {
            "location": "/#requirements",
            "text": "This guide utilizes GNU xargs. \nYou should run it on Linux, \nor use Homebrew's gxargs if \non a Mac.",
            "title": "requirements"
        },
        {
            "location": "/#demo-surgery",
            "text": "Clone an example repo for performing surgery:  $ git clone https://github.com/charlesreid1/git-commit-ectomy-example",
            "title": "demo surgery"
        },
        {
            "location": "/#how-to-make-a-fat-file",
            "text": "Use  dd  to create files by assembling \na specified number of bits.\nFor example, to create a 10 MB file:  $ dd if=/dev/urandom of=my_big_fat_file bs=1048576 count=10  Important:  You must use  /dev/urandom  with a non-zero block size.\nIf you use  /dev/zeros  then each file will be identical and git \nwill not store them separately. Then your surgery will go very badly.  Note:   1048576 = 2^20  bytes comes from\n1 KB =  2^10  bytes, and 1 MB =  2^10  KB, \nfor a total of  2^20  bytes per megabyte.  count = 10 means we make 10 1mb blocks.",
            "title": "how to make a fat file"
        },
        {
            "location": "/#make-several-fat-files",
            "text": "Crank them out. bat cat dat fat rat!  dd if=/dev/urandom of=bat bs=1048576 count=10\ndd if=/dev/urandom of=cat bs=1048576 count=10\ndd if=/dev/urandom of=dat bs=1048576 count=10\ndd if=/dev/urandom of=fat bs=1048576 count=10\ndd if=/dev/urandom of=rat bs=1048576 count=10  Make sure they are the correct size:  $ ls -lhg\n-rw-r--r--   1 staff    10M Apr 10 18:30 bat\n-rw-r--r--   1 staff    10M Apr 10 18:30 cat\n-rw-r--r--   1 staff    10M Apr 10 18:30 dat\n-rw-r--r--   1 staff    10M Apr 10 18:30 fat\n-rw-r--r--   1 staff    10M Apr 10 18:30 rat  Also make sure they are unique (hence  /dev/random  and not  /dev/zero ):  $ for i in `/bin/ls -1 *at`; do\n    md5 ${i}\ndone\n\nMD5 (bat) = 140c7d1e5a12c0eb2fefd5529250a280\nMD5 (cat) = 9345ca4033c7e42fb009e3b8014570dc\nMD5 (dat) = fadc3114fe9a70f688eba0db4e0dc7a9\nMD5 (fat) = 39e98200043e438f9070369989b2d964\nMD5 (rat) = 77b1c3077041078fd1f371b1bb7dd6e4",
            "title": "make several fat files"
        },
        {
            "location": "/#commit-files",
            "text": "Add the files to the repo in  separate commits :  for item in bat cat dat fat rat; do\n    git add ${item} && git commit ${item} -m \"Adding ${item}\"\ndone\n\ngit push origin master  I also added two dummy files  foo.txt  and  bar.txt  \nto illustrate the impact of the surgery on \ncommit history.  Now you should see everything in the commit history:   Locally in git log:  $ git log --oneline\nfd76938 Add bar.txt\n8a86eaf Add foo.txt\nc50a272 Adding rat\n423ede3 Adding fat\nb56c10b Adding dat\nbeb8b4f Adding cat\n84d894e Adding bat",
            "title": "commit files"
        },
        {
            "location": "/#prep-for-surgery",
            "text": "Use  git-forget-blob.sh \nto forget the blob. Start by downloading it:  $ wget https://tinyurl.com/git-forget-blob-sh -O git-forget-blob.sh\n$ chmod +x git-forget-blob.sh  If you are on a Mac, edit the file to change\n2 occurrences of  xargs  to  gxargs .  To use it:          ./git-forget-blob.sh <name-of-file>",
            "title": "prep for surgery"
        },
        {
            "location": "/#git-rm",
            "text": "Start by checking the size of the repo:  $ du -hs .git\n 50M    .git  Now remove  dat  using  git rm :  $ git rm dat\n$ git commit dat -m 'Removing dat'\n$ git push origin master  This, of course, does not change the size of the repo.\nIf we clone a fresh copy from Github, the size of the \nrepo is still the same:  $ du -hs .git\n 50M    .git  Git is cursed with perfect memory, and will not\nforget a large file that's been added to the repo.",
            "title": "git rm"
        },
        {
            "location": "/#git-forget-blob",
            "text": "We use  git-forget-blob.sh  to permanenty remove  dat  from the repo history and rewrite all commits\nsince the file was added:  $ ./git-forget-blob.sh dat\nCounting objects: 23, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (22/22), done.\nWriting objects: 100% (23/23), done.\nTotal 23 (delta 6), reused 0 (delta 0)\nRewrite 84d894e39d63bbea54a7b8d3d1c85c588adff7ae (1/8) (0 seconds pRewrite beb8b4f2fbb257c0d6098620de7a8df5d7dd74ed (2/8) (0 seconds pRewrite b56c10bafeb95eece9880aa1a52cfc3e4e99045e (3/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite 423ede37f4d75dbb7a1b8020c561c4d7a926a97f (4/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite c50a2724f6d722002133b04b0f63f271b861ff9c (5/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite 8a86eafbb5b669120fa1073cfa7567bbebf2fb2e (6/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite fd769386dcd32354bad57e8eb057ae8adb2b3c9b (7/8) (0 seconds passed, remaining 0 predicted)    rm 'dat'\nRewrite d0587c525a5f64d02b1cb46c04261ab285c907f9 (8/8) (0 seconds passed, remaining 0 predicted)\nRef 'refs/heads/master' was rewritten\nCounting objects: 20, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (19/19), done.\nWriting objects: 100% (20/20), done.\nTotal 20 (delta 6), reused 8 (delta 0)  Now check if it worked:  $ du -hs .git\n 40M    .git\n ```\n\nSuccess!\n\n## how it worked\n\nIf we check the git log we can see what happened - all commits involving `dat` were rewritten. Because git uses the prior commit's hash to make the next commit's hash, _we will rewrite every commit since the first time `dat` was added to the repo_:\n\nNew log:  $ git log --oneline\n3c10144 Removing dat\n6f7b8f2 Add bar.txt\n3f70da5 Add foo.txt\n5406c1a Adding rat\ndf458d0 Adding fat\n5f6e932 Adding dat\nbeb8b4f Adding cat\n84d894e Adding bat  \nCompare to the old log:  $ git log --oneline\nd0587c5 Removing dat\nfd76938 Add bar.txt\n8a86eaf Add foo.txt\nc50a272 Adding rat\n423ede3 Adding fat\nb56c10b Adding dat\nbeb8b4f Adding cat\n84d894e Adding bat  \nNote that the \"Add cat\" commit IDs are identical, \nbut every commit after `dat` was added to the repo is changed. \n\nThis is because each commit ID is computed using the hash of the prior commit, \nso if we change one commit ID in history, we change all subsequent commit IDs.\n\n## stitch the patient back up\n\nOf course, for surgeons, as for airline pilots,\nif the last step is screwed up, nothing else counts.\n\nWe asked git to forget a file, which it did, \nbut that required modifying commits in the \ngit history, which required re-calculating\ncommit hashes.\n\nAt this point, we have two separate, parallel \n`master` branches that split when our \nfile `dat` was added to the repo.\n\nIf we simply push our branch to the Github remote,\nwe will have a huge headache: every commit will have \na duplicate, the old branch and the new branch,\nand will show up side-by-side.\n\nTo do this correctly, we need to use the force\nwhen we push:  $ git push origin master --force  \nThis will ensure that Github does not keep duplicate\ncopies of all commits.\n\nHere is a screenshot of the repo on github before \nwe ran `git-forget-blob`:\n\n![Commit history before git-forget-blob](/img/before.png)\n\nAnd a screenshot of the repo after:\n\n![Commit history after git-forget-blob](/img/after.png)\n\n# tips for surgery\n\n**Size up your patient.**\n\nThis one-liner lists the 40 largest files in the repo\n(modify the tail line to change the number):  $ git rev-list --all --objects | \\\n     sed -n $(git rev-list --objects --all | \\\n     cut -f1 -d' ' | \\\n     git cat-file --batch-check | \\\n     grep blob | \\\n     sort -n -k 3 | \\\n     \\\n     tail -n40 | \\\n     \\\n     while read hash type size; do\n          echo -n \"-e s/$hash/$size/p \";\n     done) | \\\n     sort -n -k1\n```  Get your patient some insurance.  Back up any files \nyou want to remove but keep.",
            "title": "git forget blob"
        }
    ]
}