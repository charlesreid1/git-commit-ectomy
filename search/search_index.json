{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"git-commit-ectomy Perform a git-commit-ectomy to forever remove problematic commits from your repo history. This uses the git-forget-blob.sh script from @nachoparker. what is this This page covers how to perform a git-commit-ectomy. This is a procedure that removes problematic files or commits from your repository history. For example, suppose the intern adds and commits a 1 GB CSV file to your repository. After profusely apologizing, the intern removes the 1 GB file, but the damage is done, and the 1 GB file will forever bloat .git . Enter the git surgeon. A git surgeon can remove such problematic commits and get the commit history back in shape. Visit the Git College of Surgery on Github the procedure This surgical procedure can happen one of three ways: Git-Commit-Ectomy the Easy Way: Single Branch Complications: Dealing with Branches Git-Commit-Ectomy the Hard Way: Cherry Picking Oh F&!k: Please Send Backup consult with your doctor You should consult with your doctor to determine if a git-commit-ectomy is right for your repository. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1","title":"Index"},{"location":"#git-commit-ectomy","text":"Perform a git-commit-ectomy to forever remove problematic commits from your repo history. This uses the git-forget-blob.sh script from @nachoparker.","title":"git-commit-ectomy"},{"location":"#what-is-this","text":"This page covers how to perform a git-commit-ectomy. This is a procedure that removes problematic files or commits from your repository history. For example, suppose the intern adds and commits a 1 GB CSV file to your repository. After profusely apologizing, the intern removes the 1 GB file, but the damage is done, and the 1 GB file will forever bloat .git . Enter the git surgeon. A git surgeon can remove such problematic commits and get the commit history back in shape. Visit the Git College of Surgery on Github","title":"what is this"},{"location":"#the-procedure","text":"This surgical procedure can happen one of three ways: Git-Commit-Ectomy the Easy Way: Single Branch Complications: Dealing with Branches Git-Commit-Ectomy the Hard Way: Cherry Picking Oh F&!k: Please Send Backup","title":"the procedure"},{"location":"#consult-with-your-doctor","text":"You should consult with your doctor to determine if a git-commit-ectomy is right for your repository. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1","title":"consult with your doctor"},{"location":"branches/","text":"Dealing with Branches (work in progress)","title":"Complications: Dealing with Branches"},{"location":"branches/#dealing-with-branches","text":"(work in progress)","title":"Dealing with Branches"},{"location":"cherrypicking/","text":"A Git-Commit-Ectomy the Hard Way: Cherry-Picking (work in progress)","title":"Git-Commit-Ectomy the Hard Way: Cherry Picking"},{"location":"cherrypicking/#a-git-commit-ectomy-the-hard-way-cherry-picking","text":"(work in progress)","title":"A Git-Commit-Ectomy the Hard Way: Cherry-Picking"},{"location":"easy/","text":"This page walks through a demonstration git-commit-ectomy that you can perform starting with an empty git repository. It addresses the single-branch case. table of contents requirements consult with your doctor demo surgery: setup side note: how to make a fat file step 1: make several fat files commit files demo surgery: procedure prepare tools the command that doesn't work: git rm the command that does work: git forget blob how it worked stitch the patient back up tips for surgery requirements This guide utilizes GNU xargs. You should run it on Linux, or use Homebrew's gxargs if on a Mac. consult with your doctor You should consult with your doctor to determine if a git-commit-ectomy is right for you. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 When you're ready to perform the surgery, append a cut command to get the relative path to the file only , without listing the size of the file, which is what we will need when we carry out the git-commit-ectomy: $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 | \\ cut -f 2 -d' ' demo surgery: setup Clone an example repo for performing surgery. You don't need a remote repository to do the demo surgery, but we will use one in our walkthrough. $ git clone https://github.com/charlesreid1/git-commit-ectomy-example side note: how to make a fat file We will use the dd command to create files with a specified number of bits. For example, to create a 10 MB file, we can issue the command: $ dd if=/dev/urandom of=my_big_fat_file bs=1048576 count=10 Important: You must use /dev/urandom with a non-zero block size. If you use /dev/zeros then each file will be identical and git will not store them separately. Then your surgery will go very badly. Note: 1048576 = 2^20 bytes comes from the fact that 1 KB = 2^10 bytes, and 1 MB = 2^10 KB, for a total of 2^20 bytes per megabyte. count=10 means we make 10 blocks, each of size 1 MB (1048576 bytes). make several text files We start by adding some small boring text files to the repository: echo \"hello foo\" > foo.txt echo \"hello bar\" > bar.txt Now add them to the repo history: $ for item in `/bin/ls -1 *.txt`; do git add ${item} && git commit ${item} -m \"adding ${item}\" done make several fat files To demonstrate the importance of specifying the path to the large files being removed from the repository, we add several 10 MB files inside of a subdirectory. Start with the directory structure: $ mkdir data1; mkdir data2 Now create some files in each of the two directories: cd data1/ dd if=/dev/urandom of=bat bs=1048576 count=10 dd if=/dev/urandom of=cat bs=1048576 count=10 dd if=/dev/urandom of=dat bs=1048576 count=10 cd ../ cd data2/ dd if=/dev/urandom of=fat bs=1048576 count=10 dd if=/dev/urandom of=rat bs=1048576 count=10 cd ../ Now we have the following directory structure: $ tree . . \u251c\u2500\u2500 bar.txt \u251c\u2500\u2500 data1 \u2502 \u251c\u2500\u2500 bat \u2502 \u251c\u2500\u2500 cat \u2502 \u2514\u2500\u2500 dat \u251c\u2500\u2500 data2 \u2502 \u251c\u2500\u2500 fat \u2502 \u2514\u2500\u2500 rat \u2514\u2500\u2500 foo.txt $ ls -lhg data1 -rw-r--r-- 1 staff 10M Apr 10 18:30 bat -rw-r--r-- 1 staff 10M Apr 10 18:30 cat -rw-r--r-- 1 staff 10M Apr 10 18:30 dat $ ls -lhg data2 -rw-r--r-- 1 staff 10M Apr 10 18:30 fat -rw-r--r-- 1 staff 10M Apr 10 18:30 rat Also make sure they are unique (hence /dev/random and not /dev/zero ): $ for i in `/bin/ls -1 data1/*at data2/*at`; do md5 ${i} done MD5 (bat) = 140c7d1e5a12c0eb2fefd5529250a280 MD5 (cat) = 9345ca4033c7e42fb009e3b8014570dc MD5 (dat) = fadc3114fe9a70f688eba0db4e0dc7a9 MD5 (fat) = 39e98200043e438f9070369989b2d964 MD5 (rat) = 77b1c3077041078fd1f371b1bb7dd6e4 commit files Add the files to the repo in separate commits : for item in data1/bat data1/cat data1/dat data2/fat data2/rat; do git add ${item} && git commit ${item} -m \"Adding ${item}\" done Now push all the commits to the remote (this will take a while): git push origin master Now you should see everything in the commit history on Github: You should also see it locally in the git log: $ git log --oneline 902b0d8 adding rat b3376bd adding fat e2427de adding dat 25682b5 addding cat 495235a addding bat 2506d38 adding bar.txt 2eb8d13 adding foo.txt demo surgery: procedure prepare tools Use git-forget-blob.sh to forget the blob. Start by downloading it: $ wget https://tinyurl.com/git-forget-blob-mac-sh -O git-forget-blob.sh $ chmod +x git-forget-blob.sh This script will detect if you are on a Mac, and if so, will use the GNU gxargs instead of the BSD xargs . This requires GNU tools to be installed via Homebrew: brew install gnu-xargs When installing gnu-xargs , you can also add the --with-default-names flag to brew to overwrite the default BSD version of xargs (which is not compatible with the GNU version of xargs). brew install gnu-xargs --with-default-names To use the git-forget-blob.sh script: ./git-forget-blob.sh <relative-path-to-file> (See below for more detail.) the command that doesn't work: git rm Start by checking the size of the repo: $ du -hs .git 50M .git Now remove dat using git rm : $ git rm dat $ git commit dat -m 'Removing dat' $ git push origin master This, of course, does not change the size of the repo. If we clone a fresh copy from Github, the size of the repo is still the same: $ du -hs .git 50M .git Why? Because git is cursed with perfect memory, and will not forget a large file that's been added to the repo. the command that does work: git forget blob To force git to forget a large file that's been added to the repo, use the git-forget-blob.sh script to permanently remove it. Here, we remove the dat file from the repo history by modifying all commits that involve the dat file, and rewriting those commits (and, by consequence, all commits that happened after that commit). Here is how to permanently remove dat from the repo history and rewrite all commits (we specify data1/bat and not bat ): $ ./git-forget-blob.sh data1/bat Enumerating objects: 26, done. Counting objects: 100% (26/26), done. Delta compression using up to 4 threads. Compressing objects: 100% (20/20), done. Writing objects: 100% (26/26), done. Total 26 (delta 1), reused 26 (delta 1) Rewrite 495235a86e70be03ee0749733645615a093b547a (3/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite 25682b53e6c8c88328346fc2e245b5946adec6cb (4/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite e2427def6f9de095928aaecfd9fef892880e6ce8 (5/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite b3376bdc847e26bdb323408afa06112dd4c2b36d (6/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite 902b0d8e46ec8d1487ae3db3b2989dfade5dacbe (7/7) (1 seconds passed, remaining 0 predicted) rm 'data1/bat' Ref 'refs/heads/master' was rewritten Enumerating objects: 23, done. Counting objects: 100% (23/23), done. Delta compression using up to 4 threads. Compressing objects: 100% (18/18), done. Writing objects: 100% (23/23), done. Total 23 (delta 1), reused 12 (delta 0) Verify it worked by finding size of .git directory $ du -hs .git 40M .git Success! Note that if you mistakenly specify the name of the file only, without the relative path to the file, git will be looking for the file at the top level of the repository, and the file will not be found: $ ./git-forget-blob.sh bat Enumerating objects: 26, done. Counting objects: 100% (26/26), done. Delta compression using up to 4 threads. Compressing objects: 100% (21/21), done. Writing objects: 100% (26/26), done. Total 26 (delta 1), reused 0 (delta 0) bat not found in repo history how it worked If we check the git log we can see what happened - all commits involving bat were rewritten. It's important to note that when git computes the hash of each commit, it includes the hash of the prior commit - meaning, if one commit in a repository's history changes, every commit in a repository's history changes. Thus, we will rewrite every single commit since the very first commit that introduced the file we removed . Compare the old and new logs: # NEW LOG # OLD LOG $ git log --oneline $ git log --oneline 5bc57f6 adding rat 902b0d8 adding rat 3153621 adding fat b3376bd adding fat c456173 adding dat e2427de adding dat 078a5be addding cat 25682b5 addding cat 3cd75ce addding bat 495235a addding bat 2506d38 adding bar.txt 2506d38 adding bar.txt 2eb8d13 adding foo.txt 2eb8d13 adding foo.txt Note that the first two commits, which did not involve the bat file, remain identical, but every commit after 495235a (which first introduced bat) is changed. Each commit hash is computed using the prior commit hash, so once commit 495235a changes, it cascades through the entire history by changing all subsequent commit hashes. stitch the patient back up Of course, for surgeons, as for airline pilots, if the last step is screwed up, nothing else counts. We asked git to forget a file, which it did, but that required modifying git's entire commit history. At this point we have two parallel master branches - the old history and the new history. If we simply git push our branch to the Github remote, we will have a huge headache: both histories will end up on Github, our git history will contain duplicates of every commit, and the old and new history will show up side by side. To do this correctly , we need to use the force when we push, which tells the Github remote to rewrite whatever commit history it currently has with the commit history that we are pushing. $ git push origin master --force This will ensure that Github does not keep duplicate copies of all commits. Here is a screenshot of the repo on github before we ran git-forget-blob : And a screenshot of the repo after: tips for surgery Size up your patient before you start. Use the one-liner in the \"Consult with your Doctor\" section section to size up your patient before you start (modify the tail line to change the number of files). Get your patient some insurance. Back up any files you want to remove but still want to keep. Make sure you specify relative paths to file names. The git-forget-blob.sh script requires you to specify the path to the file you want to remove, relative to the top level directory of the repository . Like this: # CORRECT ./git-forget-blob.sh data/my_project/phase-1/proprietary/super_huge.file Not like this: # INCORRECT ./git-forget-blob.sh super_huge.file The long one-liner in the \"Consult with your Doctor\" section will list the largest files in the repository, with the relative path to that file (relative to the root of the repository). If you pass it a filename without a path to the file, the script will most likely complain that the file could not be found. But it may attempt to remove the file and rewrite history anyway without removing any files. If you are running git-forget-blob.sh and the size of the .git folder is not going down, it may be because you are specifying an incorrect path to the files you are trying to remove.","title":"Git-Commit-Ectomy the Easy Way: Single Branch"},{"location":"easy/#table-of-contents","text":"requirements consult with your doctor demo surgery: setup side note: how to make a fat file step 1: make several fat files commit files demo surgery: procedure prepare tools the command that doesn't work: git rm the command that does work: git forget blob how it worked stitch the patient back up tips for surgery","title":"table of contents"},{"location":"easy/#requirements","text":"This guide utilizes GNU xargs. You should run it on Linux, or use Homebrew's gxargs if on a Mac.","title":"requirements"},{"location":"easy/#consult-with-your-doctor","text":"You should consult with your doctor to determine if a git-commit-ectomy is right for you. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 When you're ready to perform the surgery, append a cut command to get the relative path to the file only , without listing the size of the file, which is what we will need when we carry out the git-commit-ectomy: $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 | \\ cut -f 2 -d' '","title":"consult with your doctor"},{"location":"easy/#demo-surgery-setup","text":"Clone an example repo for performing surgery. You don't need a remote repository to do the demo surgery, but we will use one in our walkthrough. $ git clone https://github.com/charlesreid1/git-commit-ectomy-example","title":"demo surgery: setup"},{"location":"easy/#side-note-how-to-make-a-fat-file","text":"We will use the dd command to create files with a specified number of bits. For example, to create a 10 MB file, we can issue the command: $ dd if=/dev/urandom of=my_big_fat_file bs=1048576 count=10 Important: You must use /dev/urandom with a non-zero block size. If you use /dev/zeros then each file will be identical and git will not store them separately. Then your surgery will go very badly. Note: 1048576 = 2^20 bytes comes from the fact that 1 KB = 2^10 bytes, and 1 MB = 2^10 KB, for a total of 2^20 bytes per megabyte. count=10 means we make 10 blocks, each of size 1 MB (1048576 bytes).","title":"side note: how to make a fat file"},{"location":"easy/#make-several-text-files","text":"We start by adding some small boring text files to the repository: echo \"hello foo\" > foo.txt echo \"hello bar\" > bar.txt Now add them to the repo history: $ for item in `/bin/ls -1 *.txt`; do git add ${item} && git commit ${item} -m \"adding ${item}\" done","title":"make several text files"},{"location":"easy/#make-several-fat-files","text":"To demonstrate the importance of specifying the path to the large files being removed from the repository, we add several 10 MB files inside of a subdirectory. Start with the directory structure: $ mkdir data1; mkdir data2 Now create some files in each of the two directories: cd data1/ dd if=/dev/urandom of=bat bs=1048576 count=10 dd if=/dev/urandom of=cat bs=1048576 count=10 dd if=/dev/urandom of=dat bs=1048576 count=10 cd ../ cd data2/ dd if=/dev/urandom of=fat bs=1048576 count=10 dd if=/dev/urandom of=rat bs=1048576 count=10 cd ../ Now we have the following directory structure: $ tree . . \u251c\u2500\u2500 bar.txt \u251c\u2500\u2500 data1 \u2502 \u251c\u2500\u2500 bat \u2502 \u251c\u2500\u2500 cat \u2502 \u2514\u2500\u2500 dat \u251c\u2500\u2500 data2 \u2502 \u251c\u2500\u2500 fat \u2502 \u2514\u2500\u2500 rat \u2514\u2500\u2500 foo.txt $ ls -lhg data1 -rw-r--r-- 1 staff 10M Apr 10 18:30 bat -rw-r--r-- 1 staff 10M Apr 10 18:30 cat -rw-r--r-- 1 staff 10M Apr 10 18:30 dat $ ls -lhg data2 -rw-r--r-- 1 staff 10M Apr 10 18:30 fat -rw-r--r-- 1 staff 10M Apr 10 18:30 rat Also make sure they are unique (hence /dev/random and not /dev/zero ): $ for i in `/bin/ls -1 data1/*at data2/*at`; do md5 ${i} done MD5 (bat) = 140c7d1e5a12c0eb2fefd5529250a280 MD5 (cat) = 9345ca4033c7e42fb009e3b8014570dc MD5 (dat) = fadc3114fe9a70f688eba0db4e0dc7a9 MD5 (fat) = 39e98200043e438f9070369989b2d964 MD5 (rat) = 77b1c3077041078fd1f371b1bb7dd6e4","title":"make several fat files"},{"location":"easy/#commit-files","text":"Add the files to the repo in separate commits : for item in data1/bat data1/cat data1/dat data2/fat data2/rat; do git add ${item} && git commit ${item} -m \"Adding ${item}\" done Now push all the commits to the remote (this will take a while): git push origin master Now you should see everything in the commit history on Github: You should also see it locally in the git log: $ git log --oneline 902b0d8 adding rat b3376bd adding fat e2427de adding dat 25682b5 addding cat 495235a addding bat 2506d38 adding bar.txt 2eb8d13 adding foo.txt","title":"commit files"},{"location":"easy/#demo-surgery-procedure","text":"","title":"demo surgery: procedure"},{"location":"easy/#prepare-tools","text":"Use git-forget-blob.sh to forget the blob. Start by downloading it: $ wget https://tinyurl.com/git-forget-blob-mac-sh -O git-forget-blob.sh $ chmod +x git-forget-blob.sh This script will detect if you are on a Mac, and if so, will use the GNU gxargs instead of the BSD xargs . This requires GNU tools to be installed via Homebrew: brew install gnu-xargs When installing gnu-xargs , you can also add the --with-default-names flag to brew to overwrite the default BSD version of xargs (which is not compatible with the GNU version of xargs). brew install gnu-xargs --with-default-names To use the git-forget-blob.sh script: ./git-forget-blob.sh <relative-path-to-file> (See below for more detail.)","title":"prepare tools"},{"location":"easy/#the-command-that-doesnt-work-git-rm","text":"Start by checking the size of the repo: $ du -hs .git 50M .git Now remove dat using git rm : $ git rm dat $ git commit dat -m 'Removing dat' $ git push origin master This, of course, does not change the size of the repo. If we clone a fresh copy from Github, the size of the repo is still the same: $ du -hs .git 50M .git Why? Because git is cursed with perfect memory, and will not forget a large file that's been added to the repo.","title":"the command that doesn't work: git rm"},{"location":"easy/#the-command-that-does-work-git-forget-blob","text":"To force git to forget a large file that's been added to the repo, use the git-forget-blob.sh script to permanently remove it. Here, we remove the dat file from the repo history by modifying all commits that involve the dat file, and rewriting those commits (and, by consequence, all commits that happened after that commit). Here is how to permanently remove dat from the repo history and rewrite all commits (we specify data1/bat and not bat ): $ ./git-forget-blob.sh data1/bat Enumerating objects: 26, done. Counting objects: 100% (26/26), done. Delta compression using up to 4 threads. Compressing objects: 100% (20/20), done. Writing objects: 100% (26/26), done. Total 26 (delta 1), reused 26 (delta 1) Rewrite 495235a86e70be03ee0749733645615a093b547a (3/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite 25682b53e6c8c88328346fc2e245b5946adec6cb (4/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite e2427def6f9de095928aaecfd9fef892880e6ce8 (5/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite b3376bdc847e26bdb323408afa06112dd4c2b36d (6/7) (0 seconds passed, remaining 0 predicted) rm 'data1/bat' Rewrite 902b0d8e46ec8d1487ae3db3b2989dfade5dacbe (7/7) (1 seconds passed, remaining 0 predicted) rm 'data1/bat' Ref 'refs/heads/master' was rewritten Enumerating objects: 23, done. Counting objects: 100% (23/23), done. Delta compression using up to 4 threads. Compressing objects: 100% (18/18), done. Writing objects: 100% (23/23), done. Total 23 (delta 1), reused 12 (delta 0) Verify it worked by finding size of .git directory $ du -hs .git 40M .git Success! Note that if you mistakenly specify the name of the file only, without the relative path to the file, git will be looking for the file at the top level of the repository, and the file will not be found: $ ./git-forget-blob.sh bat Enumerating objects: 26, done. Counting objects: 100% (26/26), done. Delta compression using up to 4 threads. Compressing objects: 100% (21/21), done. Writing objects: 100% (26/26), done. Total 26 (delta 1), reused 0 (delta 0) bat not found in repo history","title":"the command that does work: git forget blob"},{"location":"easy/#how-it-worked","text":"If we check the git log we can see what happened - all commits involving bat were rewritten. It's important to note that when git computes the hash of each commit, it includes the hash of the prior commit - meaning, if one commit in a repository's history changes, every commit in a repository's history changes. Thus, we will rewrite every single commit since the very first commit that introduced the file we removed . Compare the old and new logs: # NEW LOG # OLD LOG $ git log --oneline $ git log --oneline 5bc57f6 adding rat 902b0d8 adding rat 3153621 adding fat b3376bd adding fat c456173 adding dat e2427de adding dat 078a5be addding cat 25682b5 addding cat 3cd75ce addding bat 495235a addding bat 2506d38 adding bar.txt 2506d38 adding bar.txt 2eb8d13 adding foo.txt 2eb8d13 adding foo.txt Note that the first two commits, which did not involve the bat file, remain identical, but every commit after 495235a (which first introduced bat) is changed. Each commit hash is computed using the prior commit hash, so once commit 495235a changes, it cascades through the entire history by changing all subsequent commit hashes.","title":"how it worked"},{"location":"easy/#stitch-the-patient-back-up","text":"Of course, for surgeons, as for airline pilots, if the last step is screwed up, nothing else counts. We asked git to forget a file, which it did, but that required modifying git's entire commit history. At this point we have two parallel master branches - the old history and the new history. If we simply git push our branch to the Github remote, we will have a huge headache: both histories will end up on Github, our git history will contain duplicates of every commit, and the old and new history will show up side by side. To do this correctly , we need to use the force when we push, which tells the Github remote to rewrite whatever commit history it currently has with the commit history that we are pushing. $ git push origin master --force This will ensure that Github does not keep duplicate copies of all commits. Here is a screenshot of the repo on github before we ran git-forget-blob : And a screenshot of the repo after:","title":"stitch the patient back up"},{"location":"easy/#tips-for-surgery","text":"Size up your patient before you start. Use the one-liner in the \"Consult with your Doctor\" section section to size up your patient before you start (modify the tail line to change the number of files). Get your patient some insurance. Back up any files you want to remove but still want to keep. Make sure you specify relative paths to file names. The git-forget-blob.sh script requires you to specify the path to the file you want to remove, relative to the top level directory of the repository . Like this: # CORRECT ./git-forget-blob.sh data/my_project/phase-1/proprietary/super_huge.file Not like this: # INCORRECT ./git-forget-blob.sh super_huge.file The long one-liner in the \"Consult with your Doctor\" section will list the largest files in the repository, with the relative path to that file (relative to the root of the repository). If you pass it a filename without a path to the file, the script will most likely complain that the file could not be found. But it may attempt to remove the file and rewrite history anyway without removing any files. If you are running git-forget-blob.sh and the size of the .git folder is not going down, it may be because you are specifying an incorrect path to the files you are trying to remove.","title":"tips for surgery"},{"location":"ohfk/","text":"Oh F&!k: Please Send Backup When you use the single-branch procedure instead of the multi-branch procedure... this is what happens. Possible complications to all of this include: Clueless individuals who do not follow basic instructions and continue to push old versions of the repo, which can be proactively addressed by branch protection but sometimes must also be retroactively dealt with; Accidentally carrying out the procedure multiple times, resulting in 2, 3, 4, even 5 copies of each commit, and the duplicate commits simply refuse to die A confusing, tangled, messy commit history that is completely uninterpretable the first case In the first case, start by turning on branch protection, then clone a fresh copy of the repository (complete with all the duplicate commits) the second case In the second case, find the branch you want to keep, then rebase or cherry pick any missing commits onto it. Now remove all the other branches. If these remote branches are not being removed, make sure you have your syntax right, and make sure you're using the --force command. git push remote_name :branch_to_delete For example, if I want to delete the branch master because I am creating a branch new_master with improved/cleaned history, I would do the following: git checkout <hash-of-commit-to-split-at> # starting point for new branch git branch new_master # create new branch git checkout new_master # switch to new branch ...make some commits... ...rebase some stuff... ...cherry pick some stuff... ...now new branch has a long and totally different commit history from master... git push origin new_master # push all the new stuff to remote \"origin\" git branch -D master # delete git branch master git push origin :master # propagate deletion of branch \"master\" to remote \"origin\" the third case If you have an absolute clusterfuck of a commit history, you need a gifted surgeon. The more gifted the surgeon, the more of your repo history you'll be able to retain. If you want to perform surgery with a battle axe, you can simply leapfrog a whole series of complicated splits, merges, rebases, and just jump to a point in the repo where things are saner and calmer. Clone a fresh copy of the repo, and checkout the commit where the clusterfuck is over, when things are saner and calmer: git checkout <commit-hash> Now, you're going to copy every single file in that folder into your current (cluster-fucked) repo, exactly, word for word, character for character. If you have extra files, those are okay. If you have deleted files that are in the saner, calmer commit state, you must add them back in. Once everything matches, commit your changes. Tag this commit as your \"Stargate\" - this is the commit that will allow you to rebase from one branch to the other. The repo will be in exactly the same state (with exception of extra files) between these two commits, so changes to one commit can be applied to the other just fine. Note that you will lose all information about commits that are not rebased or cherry picked, i.e., all the commits that were involved with the clusterfuck.","title":"Oh F&!k: Please Send Backup"},{"location":"ohfk/#oh-fk-please-send-backup","text":"When you use the single-branch procedure instead of the multi-branch procedure... this is what happens. Possible complications to all of this include: Clueless individuals who do not follow basic instructions and continue to push old versions of the repo, which can be proactively addressed by branch protection but sometimes must also be retroactively dealt with; Accidentally carrying out the procedure multiple times, resulting in 2, 3, 4, even 5 copies of each commit, and the duplicate commits simply refuse to die A confusing, tangled, messy commit history that is completely uninterpretable","title":"Oh F&amp;!k: Please Send Backup"},{"location":"ohfk/#the-first-case","text":"In the first case, start by turning on branch protection, then clone a fresh copy of the repository (complete with all the duplicate commits)","title":"the first case"},{"location":"ohfk/#the-second-case","text":"In the second case, find the branch you want to keep, then rebase or cherry pick any missing commits onto it. Now remove all the other branches. If these remote branches are not being removed, make sure you have your syntax right, and make sure you're using the --force command. git push remote_name :branch_to_delete For example, if I want to delete the branch master because I am creating a branch new_master with improved/cleaned history, I would do the following: git checkout <hash-of-commit-to-split-at> # starting point for new branch git branch new_master # create new branch git checkout new_master # switch to new branch ...make some commits... ...rebase some stuff... ...cherry pick some stuff... ...now new branch has a long and totally different commit history from master... git push origin new_master # push all the new stuff to remote \"origin\" git branch -D master # delete git branch master git push origin :master # propagate deletion of branch \"master\" to remote \"origin\"","title":"the second case"},{"location":"ohfk/#the-third-case","text":"If you have an absolute clusterfuck of a commit history, you need a gifted surgeon. The more gifted the surgeon, the more of your repo history you'll be able to retain. If you want to perform surgery with a battle axe, you can simply leapfrog a whole series of complicated splits, merges, rebases, and just jump to a point in the repo where things are saner and calmer. Clone a fresh copy of the repo, and checkout the commit where the clusterfuck is over, when things are saner and calmer: git checkout <commit-hash> Now, you're going to copy every single file in that folder into your current (cluster-fucked) repo, exactly, word for word, character for character. If you have extra files, those are okay. If you have deleted files that are in the saner, calmer commit state, you must add them back in. Once everything matches, commit your changes. Tag this commit as your \"Stargate\" - this is the commit that will allow you to rebase from one branch to the other. The repo will be in exactly the same state (with exception of extra files) between these two commits, so changes to one commit can be applied to the other just fine. Note that you will lose all information about commits that are not rebased or cherry picked, i.e., all the commits that were involved with the clusterfuck.","title":"the third case"}]}