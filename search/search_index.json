{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"git-commit-ectomy Perform a git-commit-ectomy to forever remove problematic commits from your repo history. This uses the git-forget-blob.sh script from @nachoparker. what is this This page covers how to perform a git-commit-ectomy. This is a procedure that removes problematic files or commits from your repository history. For example, suppose the intern adds and commits a 1 GB CSV file to your repository. After profusely apologizing, the intern removes the 1 GB file, but the damage is done, and the 1 GB file will forever bloat .git . Enter the git surgeon. A git surgeon can remove such problematic commits and get the commit history back in shape. Visit the Git College of Surgery on Github the procedure This surgical procedure can happen one of three ways: Git-Commit-Ectomy the Easy Way: Single Branch Complications: Dealing with Branches Git-Commit-Ectomy the Hard Way: Cherry Picking Oh F&!k: Please Send Backup consult with your doctor You should consult with your doctor to determine if a git-commit-ectomy is right for your repository. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1","title":"Index"},{"location":"#git-commit-ectomy","text":"Perform a git-commit-ectomy to forever remove problematic commits from your repo history. This uses the git-forget-blob.sh script from @nachoparker.","title":"git-commit-ectomy"},{"location":"#what-is-this","text":"This page covers how to perform a git-commit-ectomy. This is a procedure that removes problematic files or commits from your repository history. For example, suppose the intern adds and commits a 1 GB CSV file to your repository. After profusely apologizing, the intern removes the 1 GB file, but the damage is done, and the 1 GB file will forever bloat .git . Enter the git surgeon. A git surgeon can remove such problematic commits and get the commit history back in shape. Visit the Git College of Surgery on Github","title":"what is this"},{"location":"#the-procedure","text":"This surgical procedure can happen one of three ways: Git-Commit-Ectomy the Easy Way: Single Branch Complications: Dealing with Branches Git-Commit-Ectomy the Hard Way: Cherry Picking Oh F&!k: Please Send Backup","title":"the procedure"},{"location":"#consult-with-your-doctor","text":"You should consult with your doctor to determine if a git-commit-ectomy is right for your repository. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1","title":"consult with your doctor"},{"location":"branches/","text":"Dealing with Branches (work in progress)","title":"Complications: Dealing with Branches"},{"location":"branches/#dealing-with-branches","text":"(work in progress)","title":"Dealing with Branches"},{"location":"cherrypicking/","text":"A Git-Commit-Ectomy the Hard Way: Cherry-Picking (work in progress)","title":"Git-Commit-Ectomy the Hard Way: Cherry Picking"},{"location":"cherrypicking/#a-git-commit-ectomy-the-hard-way-cherry-picking","text":"(work in progress)","title":"A Git-Commit-Ectomy the Hard Way: Cherry-Picking"},{"location":"easy/","text":"This page walks through a demonstration git-commit-ectomy that you can perform starting with an empty git repository. It addresses the single-branch case. table of contents requirements consult with your doctor demo surgery: setup side note: how to make a fat file step 1: make several fat files commit files demo surgery: procedure prepare tools the command that doesn't work: git rm the command that does work: git forget blob how it worked stitch the patient back up tips for surgery requirements This guide utilizes GNU xargs. You should run it on Linux, or use Homebrew's gxargs if on a Mac. consult with your doctor You should consult with your doctor to determine if a git-commit-ectomy is right for you. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 When you're ready to perform the surgery, append a cut command to get the relative path to the file only , without listing the size of the file, which is what we will need when we carry out the git-commit-ectomy: $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 | \\ cut -f 2 -d' ' demo surgery: setup Clone an example repo for performing surgery: $ git clone https://github.com/charlesreid1/git-commit-ectomy-example side note: how to make a fat file Use dd to create files by assembling a specified number of bits. For example, to create a 10 MB file: $ dd if=/dev/urandom of=my_big_fat_file bs=1048576 count=10 Important: You must use /dev/urandom with a non-zero block size. If you use /dev/zeros then each file will be identical and git will not store them separately. Then your surgery will go very badly. Note: 1048576 = 2^20 bytes comes from 1 KB = 2^10 bytes, and 1 MB = 2^10 KB, for a total of 2^20 bytes per megabyte. count = 10 means we make 10 1mb blocks. make several fat files Crank them out. bat cat dat fat rat! dd if=/dev/urandom of=bat bs=1048576 count=10 dd if=/dev/urandom of=cat bs=1048576 count=10 dd if=/dev/urandom of=dat bs=1048576 count=10 dd if=/dev/urandom of=fat bs=1048576 count=10 dd if=/dev/urandom of=rat bs=1048576 count=10 Make sure they are the correct size: $ ls -lhg -rw-r--r-- 1 staff 10M Apr 10 18:30 bat -rw-r--r-- 1 staff 10M Apr 10 18:30 cat -rw-r--r-- 1 staff 10M Apr 10 18:30 dat -rw-r--r-- 1 staff 10M Apr 10 18:30 fat -rw-r--r-- 1 staff 10M Apr 10 18:30 rat Also make sure they are unique (hence /dev/random and not /dev/zero ): $ for i in `/bin/ls -1 *at`; do md5 ${i} done MD5 (bat) = 140c7d1e5a12c0eb2fefd5529250a280 MD5 (cat) = 9345ca4033c7e42fb009e3b8014570dc MD5 (dat) = fadc3114fe9a70f688eba0db4e0dc7a9 MD5 (fat) = 39e98200043e438f9070369989b2d964 MD5 (rat) = 77b1c3077041078fd1f371b1bb7dd6e4 commit files Add the files to the repo in separate commits : for item in bat cat dat fat rat; do git add ${item} && git commit ${item} -m \"Adding ${item}\" done git push origin master I also added two dummy files foo.txt and bar.txt to illustrate the impact of the surgery on commit history. Now you should see everything in the commit history: Locally in git log: $ git log --oneline fd76938 Add bar.txt 8a86eaf Add foo.txt c50a272 Adding rat 423ede3 Adding fat b56c10b Adding dat beb8b4f Adding cat 84d894e Adding bat demo surgery: procedure prepare tools Use git-forget-blob.sh to forget the blob. Start by downloading it: $ wget https://tinyurl.com/git-forget-blob-mac-sh -O git-forget-blob.sh $ chmod +x git-forget-blob.sh This script will detect if you are on a Mac, and if so, will use the GNU gxargs instead of the BSD xargs . This requires GNU tools to be installed via Homebrew: brew install gnu-sed Optional: can use --with-default-names to overwrite BSD versions with GNU versions. brew install gnu-sed --with-default-names To use the script: ./git-forget-blob.sh <relative-path-to-file> the command that doesn't work: git rm Start by checking the size of the repo: $ du -hs .git 50M .git Now remove dat using git rm : $ git rm dat $ git commit dat -m 'Removing dat' $ git push origin master This, of course, does not change the size of the repo. If we clone a fresh copy from Github, the size of the repo is still the same: $ du -hs .git 50M .git Git is cursed with perfect memory, and will not forget a large file that's been added to the repo. the command that does work: git forget blob We use git-forget-blob.sh to permanenty remove dat from the repo history and rewrite all commits since the file was added: $ ./git-forget-blob.sh dat Counting objects: 23, done. Delta compression using up to 4 threads. Compressing objects: 100% (22/22), done. Writing objects: 100% (23/23), done. Total 23 (delta 6), reused 0 (delta 0) Rewrite 84d894e39d63bbea54a7b8d3d1c85c588adff7ae (1/8) (0 seconds pRewrite beb8b4f2fbb257c0d6098620de7a8df5d7dd74ed (2/8) (0 seconds pRewrite b56c10bafeb95eece9880aa1a52cfc3e4e99045e (3/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite 423ede37f4d75dbb7a1b8020c561c4d7a926a97f (4/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite c50a2724f6d722002133b04b0f63f271b861ff9c (5/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite 8a86eafbb5b669120fa1073cfa7567bbebf2fb2e (6/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite fd769386dcd32354bad57e8eb057ae8adb2b3c9b (7/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite d0587c525a5f64d02b1cb46c04261ab285c907f9 (8/8) (0 seconds passed, remaining 0 predicted) Ref 'refs/heads/master' was rewritten Counting objects: 20, done. Delta compression using up to 4 threads. Compressing objects: 100% (19/19), done. Writing objects: 100% (20/20), done. Total 20 (delta 6), reused 8 (delta 0) REMEMBER: if dat is in a path in the repo, like path/to/dat , you have to pass the full path : $ ./git-forget-blob.sh path/to/dat Now check if it worked: $ du -hs .git 40M .git Success! how it worked If we check the git log we can see what happened - all commits involving dat were rewritten. Because git uses the prior commit's hash to make the next commit's hash, we will rewrite every commit since the first time dat was added to the repo : New log: $ git log --oneline 3c10144 Removing dat 6f7b8f2 Add bar.txt 3f70da5 Add foo.txt 5406c1a Adding rat df458d0 Adding fat 5f6e932 Adding dat beb8b4f Adding cat 84d894e Adding bat Compare to the old log: $ git log --oneline d0587c5 Removing dat fd76938 Add bar.txt 8a86eaf Add foo.txt c50a272 Adding rat 423ede3 Adding fat b56c10b Adding dat beb8b4f Adding cat 84d894e Adding bat Note that the \"Add cat\" commit IDs are identical, but every commit after dat was added to the repo is changed. This is because each commit ID is computed using the hash of the prior commit, so if we change one commit ID in history, we change all subsequent commit IDs. stitch the patient back up Of course, for surgeons, as for airline pilots, if the last step is screwed up, nothing else counts. We asked git to forget a file, which it did, but that required modifying commits in the git history, which required re-calculating commit hashes. At this point, we have two separate, parallel master branches that split when our file dat was added to the repo. If we simply push our branch to the Github remote, we will have a huge headache: every commit will have a duplicate, the old branch and the new branch, and will show up side-by-side. To do this correctly, we need to use the force when we push: $ git push origin master --force This will ensure that Github does not keep duplicate copies of all commits. Here is a screenshot of the repo on github before we ran git-forget-blob : And a screenshot of the repo after: tips for surgery Pass the relative path to the file to git-forget-blob.sh . This is very important!!! The long one-liner in the \"Consult with your Doctor\" section will list the largest files in the repository, with the relative path to that file in the repository. If you pass it a filename without a path to the file, the script will most likely complain that the file could not be found. If you are running git-forget-blob.sh and the size of the .git folder is not going down, it may be because you are specifying an incorrect path to the files you are trying to remove. Size up your patient before you start. Use the one-liner in the \"Consult with your Doctor\" section section to size up your patient before you start (modify the tail line to change the number of files). Get your patient some insurance. Back up any files you want to remove but still want to keep. Make sure you specify relative paths to file names. The git-forget-blob.sh script uses blobs in the .git directory. These blobs contain relative path information, so you must specify the relative path to the file, like this: # CORRECT ./git-forget-blob.sh data/my_project/phase-1/proprietary/super_huge.file not this: # INCORRECT ./git-forget-blob.sh super_huge.file","title":"Git-Commit-Ectomy the Easy Way: Single Branch"},{"location":"easy/#table-of-contents","text":"requirements consult with your doctor demo surgery: setup side note: how to make a fat file step 1: make several fat files commit files demo surgery: procedure prepare tools the command that doesn't work: git rm the command that does work: git forget blob how it worked stitch the patient back up tips for surgery","title":"table of contents"},{"location":"easy/#requirements","text":"This guide utilizes GNU xargs. You should run it on Linux, or use Homebrew's gxargs if on a Mac.","title":"requirements"},{"location":"easy/#consult-with-your-doctor","text":"You should consult with your doctor to determine if a git-commit-ectomy is right for you. This one-liner lists the 40 largest files in the repo (modify the tail line to change the number): $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 When you're ready to perform the surgery, append a cut command to get the relative path to the file only , without listing the size of the file, which is what we will need when we carry out the git-commit-ectomy: $ git rev-list --all --objects | \\ sed -n $(git rev-list --objects --all | \\ cut -f1 -d' ' | \\ git cat-file --batch-check | \\ grep blob | \\ sort -n -k 3 | \\ \\ tail -n40 | \\ \\ while read hash type size; do echo -n \"-e s/$hash/$size/p \"; done) | \\ sort -n -r -k1 | \\ cut -f 2 -d' '","title":"consult with your doctor"},{"location":"easy/#demo-surgery-setup","text":"Clone an example repo for performing surgery: $ git clone https://github.com/charlesreid1/git-commit-ectomy-example","title":"demo surgery: setup"},{"location":"easy/#side-note-how-to-make-a-fat-file","text":"Use dd to create files by assembling a specified number of bits. For example, to create a 10 MB file: $ dd if=/dev/urandom of=my_big_fat_file bs=1048576 count=10 Important: You must use /dev/urandom with a non-zero block size. If you use /dev/zeros then each file will be identical and git will not store them separately. Then your surgery will go very badly. Note: 1048576 = 2^20 bytes comes from 1 KB = 2^10 bytes, and 1 MB = 2^10 KB, for a total of 2^20 bytes per megabyte. count = 10 means we make 10 1mb blocks.","title":"side note: how to make a fat file"},{"location":"easy/#make-several-fat-files","text":"Crank them out. bat cat dat fat rat! dd if=/dev/urandom of=bat bs=1048576 count=10 dd if=/dev/urandom of=cat bs=1048576 count=10 dd if=/dev/urandom of=dat bs=1048576 count=10 dd if=/dev/urandom of=fat bs=1048576 count=10 dd if=/dev/urandom of=rat bs=1048576 count=10 Make sure they are the correct size: $ ls -lhg -rw-r--r-- 1 staff 10M Apr 10 18:30 bat -rw-r--r-- 1 staff 10M Apr 10 18:30 cat -rw-r--r-- 1 staff 10M Apr 10 18:30 dat -rw-r--r-- 1 staff 10M Apr 10 18:30 fat -rw-r--r-- 1 staff 10M Apr 10 18:30 rat Also make sure they are unique (hence /dev/random and not /dev/zero ): $ for i in `/bin/ls -1 *at`; do md5 ${i} done MD5 (bat) = 140c7d1e5a12c0eb2fefd5529250a280 MD5 (cat) = 9345ca4033c7e42fb009e3b8014570dc MD5 (dat) = fadc3114fe9a70f688eba0db4e0dc7a9 MD5 (fat) = 39e98200043e438f9070369989b2d964 MD5 (rat) = 77b1c3077041078fd1f371b1bb7dd6e4","title":"make several fat files"},{"location":"easy/#commit-files","text":"Add the files to the repo in separate commits : for item in bat cat dat fat rat; do git add ${item} && git commit ${item} -m \"Adding ${item}\" done git push origin master I also added two dummy files foo.txt and bar.txt to illustrate the impact of the surgery on commit history. Now you should see everything in the commit history: Locally in git log: $ git log --oneline fd76938 Add bar.txt 8a86eaf Add foo.txt c50a272 Adding rat 423ede3 Adding fat b56c10b Adding dat beb8b4f Adding cat 84d894e Adding bat","title":"commit files"},{"location":"easy/#demo-surgery-procedure","text":"","title":"demo surgery: procedure"},{"location":"easy/#prepare-tools","text":"Use git-forget-blob.sh to forget the blob. Start by downloading it: $ wget https://tinyurl.com/git-forget-blob-mac-sh -O git-forget-blob.sh $ chmod +x git-forget-blob.sh This script will detect if you are on a Mac, and if so, will use the GNU gxargs instead of the BSD xargs . This requires GNU tools to be installed via Homebrew: brew install gnu-sed Optional: can use --with-default-names to overwrite BSD versions with GNU versions. brew install gnu-sed --with-default-names To use the script: ./git-forget-blob.sh <relative-path-to-file>","title":"prepare tools"},{"location":"easy/#the-command-that-doesnt-work-git-rm","text":"Start by checking the size of the repo: $ du -hs .git 50M .git Now remove dat using git rm : $ git rm dat $ git commit dat -m 'Removing dat' $ git push origin master This, of course, does not change the size of the repo. If we clone a fresh copy from Github, the size of the repo is still the same: $ du -hs .git 50M .git Git is cursed with perfect memory, and will not forget a large file that's been added to the repo.","title":"the command that doesn't work: git rm"},{"location":"easy/#the-command-that-does-work-git-forget-blob","text":"We use git-forget-blob.sh to permanenty remove dat from the repo history and rewrite all commits since the file was added: $ ./git-forget-blob.sh dat Counting objects: 23, done. Delta compression using up to 4 threads. Compressing objects: 100% (22/22), done. Writing objects: 100% (23/23), done. Total 23 (delta 6), reused 0 (delta 0) Rewrite 84d894e39d63bbea54a7b8d3d1c85c588adff7ae (1/8) (0 seconds pRewrite beb8b4f2fbb257c0d6098620de7a8df5d7dd74ed (2/8) (0 seconds pRewrite b56c10bafeb95eece9880aa1a52cfc3e4e99045e (3/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite 423ede37f4d75dbb7a1b8020c561c4d7a926a97f (4/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite c50a2724f6d722002133b04b0f63f271b861ff9c (5/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite 8a86eafbb5b669120fa1073cfa7567bbebf2fb2e (6/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite fd769386dcd32354bad57e8eb057ae8adb2b3c9b (7/8) (0 seconds passed, remaining 0 predicted) rm 'dat' Rewrite d0587c525a5f64d02b1cb46c04261ab285c907f9 (8/8) (0 seconds passed, remaining 0 predicted) Ref 'refs/heads/master' was rewritten Counting objects: 20, done. Delta compression using up to 4 threads. Compressing objects: 100% (19/19), done. Writing objects: 100% (20/20), done. Total 20 (delta 6), reused 8 (delta 0) REMEMBER: if dat is in a path in the repo, like path/to/dat , you have to pass the full path : $ ./git-forget-blob.sh path/to/dat Now check if it worked: $ du -hs .git 40M .git Success!","title":"the command that does work: git forget blob"},{"location":"easy/#how-it-worked","text":"If we check the git log we can see what happened - all commits involving dat were rewritten. Because git uses the prior commit's hash to make the next commit's hash, we will rewrite every commit since the first time dat was added to the repo : New log: $ git log --oneline 3c10144 Removing dat 6f7b8f2 Add bar.txt 3f70da5 Add foo.txt 5406c1a Adding rat df458d0 Adding fat 5f6e932 Adding dat beb8b4f Adding cat 84d894e Adding bat Compare to the old log: $ git log --oneline d0587c5 Removing dat fd76938 Add bar.txt 8a86eaf Add foo.txt c50a272 Adding rat 423ede3 Adding fat b56c10b Adding dat beb8b4f Adding cat 84d894e Adding bat Note that the \"Add cat\" commit IDs are identical, but every commit after dat was added to the repo is changed. This is because each commit ID is computed using the hash of the prior commit, so if we change one commit ID in history, we change all subsequent commit IDs.","title":"how it worked"},{"location":"easy/#stitch-the-patient-back-up","text":"Of course, for surgeons, as for airline pilots, if the last step is screwed up, nothing else counts. We asked git to forget a file, which it did, but that required modifying commits in the git history, which required re-calculating commit hashes. At this point, we have two separate, parallel master branches that split when our file dat was added to the repo. If we simply push our branch to the Github remote, we will have a huge headache: every commit will have a duplicate, the old branch and the new branch, and will show up side-by-side. To do this correctly, we need to use the force when we push: $ git push origin master --force This will ensure that Github does not keep duplicate copies of all commits. Here is a screenshot of the repo on github before we ran git-forget-blob : And a screenshot of the repo after:","title":"stitch the patient back up"},{"location":"easy/#tips-for-surgery","text":"Pass the relative path to the file to git-forget-blob.sh . This is very important!!! The long one-liner in the \"Consult with your Doctor\" section will list the largest files in the repository, with the relative path to that file in the repository. If you pass it a filename without a path to the file, the script will most likely complain that the file could not be found. If you are running git-forget-blob.sh and the size of the .git folder is not going down, it may be because you are specifying an incorrect path to the files you are trying to remove. Size up your patient before you start. Use the one-liner in the \"Consult with your Doctor\" section section to size up your patient before you start (modify the tail line to change the number of files). Get your patient some insurance. Back up any files you want to remove but still want to keep. Make sure you specify relative paths to file names. The git-forget-blob.sh script uses blobs in the .git directory. These blobs contain relative path information, so you must specify the relative path to the file, like this: # CORRECT ./git-forget-blob.sh data/my_project/phase-1/proprietary/super_huge.file not this: # INCORRECT ./git-forget-blob.sh super_huge.file","title":"tips for surgery"},{"location":"ohfk/","text":"Oh F&!k: Please Send Backup When you use the single-branch procedure instead of the multi-branch procedure... this is what happens. Possible complications to all of this include: Clueless individuals who do not follow basic instructions and continue to push old versions of the repo, which can be proactively addressed by branch protection but sometimes must also be retroactively dealt with; Accidentally carrying out the procedure multiple times, resulting in 2, 3, 4, even 5 copies of each commit, and the duplicate commits simply refuse to die A confusing, tangled, messy commit history that is completely uninterpretable the first case In the first case, start by turning on branch protection, then clone a fresh copy of the repository (complete with all the duplicate commits) the second case In the second case, find the branch you want to keep, then rebase or cherry pick any missing commits onto it. Now remove all the other branches. If these remote branches are not being removed, make sure you have your syntax right, and make sure you're using the --force command. git push remote_name :branch_to_delete For example, if I want to delete the branch master because I am creating a branch new_master with improved/cleaned history, I would do the following: git checkout <hash-of-commit-to-split-at> # starting point for new branch git branch new_master # create new branch git checkout new_master # switch to new branch ...make some commits... ...rebase some stuff... ...cherry pick some stuff... ...now new branch has a long and totally different commit history from master... git push origin new_master # push all the new stuff to remote \"origin\" git branch -D master # delete git branch master git push origin :master # propagate deletion of branch \"master\" to remote \"origin\" the third case If you have an absolute clusterfuck of a commit history, you need a gifted surgeon. The more gifted the surgeon, the more of your repo history you'll be able to retain. If you want to perform surgery with a battle axe, you can simply leapfrog a whole series of complicated splits, merges, rebases, and just jump to a point in the repo where things are saner and calmer. Clone a fresh copy of the repo, and checkout the commit where the clusterfuck is over, when things are saner and calmer: git checkout <commit-hash> Now, you're going to copy every single file in that folder into your current (cluster-fucked) repo, exactly, word for word, character for character. If you have extra files, those are okay. If you have deleted files that are in the saner, calmer commit state, you must add them back in. Once everything matches, commit your changes. Tag this commit as your \"Stargate\" - this is the commit that will allow you to rebase from one branch to the other. The repo will be in exactly the same state (with exception of extra files) between these two commits, so changes to one commit can be applied to the other just fine. Note that you will lose all information about commits that are not rebased or cherry picked, i.e., all the commits that were involved with the clusterfuck.","title":"Oh F&!k: Please Send Backup"},{"location":"ohfk/#oh-fk-please-send-backup","text":"When you use the single-branch procedure instead of the multi-branch procedure... this is what happens. Possible complications to all of this include: Clueless individuals who do not follow basic instructions and continue to push old versions of the repo, which can be proactively addressed by branch protection but sometimes must also be retroactively dealt with; Accidentally carrying out the procedure multiple times, resulting in 2, 3, 4, even 5 copies of each commit, and the duplicate commits simply refuse to die A confusing, tangled, messy commit history that is completely uninterpretable","title":"Oh F&amp;!k: Please Send Backup"},{"location":"ohfk/#the-first-case","text":"In the first case, start by turning on branch protection, then clone a fresh copy of the repository (complete with all the duplicate commits)","title":"the first case"},{"location":"ohfk/#the-second-case","text":"In the second case, find the branch you want to keep, then rebase or cherry pick any missing commits onto it. Now remove all the other branches. If these remote branches are not being removed, make sure you have your syntax right, and make sure you're using the --force command. git push remote_name :branch_to_delete For example, if I want to delete the branch master because I am creating a branch new_master with improved/cleaned history, I would do the following: git checkout <hash-of-commit-to-split-at> # starting point for new branch git branch new_master # create new branch git checkout new_master # switch to new branch ...make some commits... ...rebase some stuff... ...cherry pick some stuff... ...now new branch has a long and totally different commit history from master... git push origin new_master # push all the new stuff to remote \"origin\" git branch -D master # delete git branch master git push origin :master # propagate deletion of branch \"master\" to remote \"origin\"","title":"the second case"},{"location":"ohfk/#the-third-case","text":"If you have an absolute clusterfuck of a commit history, you need a gifted surgeon. The more gifted the surgeon, the more of your repo history you'll be able to retain. If you want to perform surgery with a battle axe, you can simply leapfrog a whole series of complicated splits, merges, rebases, and just jump to a point in the repo where things are saner and calmer. Clone a fresh copy of the repo, and checkout the commit where the clusterfuck is over, when things are saner and calmer: git checkout <commit-hash> Now, you're going to copy every single file in that folder into your current (cluster-fucked) repo, exactly, word for word, character for character. If you have extra files, those are okay. If you have deleted files that are in the saner, calmer commit state, you must add them back in. Once everything matches, commit your changes. Tag this commit as your \"Stargate\" - this is the commit that will allow you to rebase from one branch to the other. The repo will be in exactly the same state (with exception of extra files) between these two commits, so changes to one commit can be applied to the other just fine. Note that you will lose all information about commits that are not rebased or cherry picked, i.e., all the commits that were involved with the clusterfuck.","title":"the third case"}]}